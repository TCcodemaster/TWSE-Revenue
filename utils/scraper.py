# åœ¨ utils/scraper.py æ–‡ä»¶ä¸­ä¿®æ”¹é€²åº¦è¿½è¹¤ç›¸é—œä»£ç¢¼

import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import logging
import os
import json
import time
from functools import lru_cache
import random
import threading
from utils.database import Database
# å°å…¥æ–°çš„é€²åº¦è¿½è¹¤å™¨
from utils.progress_tracker import initialize, update_company, increment, complete, error, get_status

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å»ºç«‹å¿«å–ç›®éŒ„
CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'cache')
os.makedirs(CACHE_DIR, exist_ok=True)

#  å»ºç«‹ db å¯¦ä¾‹
db_path = os.path.join(os.environ.get("DATABASE_DIR", "./data"), "data.db")
db = Database(db_path)
# æ·»åŠ è¯·æ±‚é™åˆ¶å’Œé€€é¿ç­–ç•¥
REQUEST_DELAY = 0.001  # åŸºæœ¬å»¶é²æ™‚é–“ (ç§’)
MAX_WORKERS = 8  # é™ä½ä¸¦è¡Œè«‹æ±‚æ•¸

# æ·»åŠ è‡ªé©æ‡‰ä¸¦è¡Œæ©Ÿåˆ¶
class AdaptiveThrottler:
    def __init__(self, initial_workers=3, min_workers=1, max_workers=8):
        self.current_workers = initial_workers
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.success_count = 0
        self.failure_count = 0
        self.lock = threading.Lock()
    
    def report_success(self):
        with self.lock:
            self.success_count += 1
            # é€£çºŒæˆåŠŸ10æ¬¡ï¼Œå¢åŠ workeræ•¸
            if self.success_count >= 10 and self.current_workers < self.max_workers:
                self.current_workers = min(self.current_workers + 1, self.max_workers)
                self.success_count = 0
                logger.info(f"å¢åŠ ä¸¦è¡Œæ•¸åˆ° {self.current_workers}")
    
    def report_failure(self):
        with self.lock:
            self.failure_count += 1
            self.success_count = 0
            # å¤±æ•—ä¸€æ¬¡å°±æ¸›å°‘workeræ•¸
            if self.failure_count >= 1 and self.current_workers > self.min_workers:
                self.current_workers = max(self.current_workers - 1, self.min_workers)
                self.failure_count = 0
                logger.info(f"é™ä½ä¸¦è¡Œæ•¸åˆ° {self.current_workers}")
    
    def get_current_workers(self):
        with self.lock:
            return self.current_workers

# åˆå§‹åŒ–throttler
throttler = AdaptiveThrottler(initial_workers=3)

def get_cache_path(company_id, year, month):
    """ç²å–å¿«å–æ–‡ä»¶è·¯å¾‘ï¼Œä½¿ç”¨å­ç›®éŒ„çµ„ç¹”ç·©å­˜"""
    # å‰µå»ºæŒ‰å¹´ä»½å’Œå…¬å¸åˆ†é¡çš„å­ç›®éŒ„ï¼Œæ¸›å°‘å–®ç›®éŒ„ä¸‹çš„æ–‡ä»¶æ•¸é‡
    company_dir = os.path.join(CACHE_DIR, company_id)
    year_dir = os.path.join(company_dir, str(year))
    os.makedirs(year_dir, exist_ok=True)
    return os.path.join(year_dir, f"{month:02d}.json")

def validate_cache_data(data):
    """é©—è­‰å¿«å–æ•¸æ“šçš„æœ‰æ•ˆæ€§"""
    required_fields = ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'ç•¶æœˆç‡Ÿæ”¶', 'ä¸Šæœˆç‡Ÿæ”¶', 'å»å¹´ç•¶æœˆç‡Ÿæ”¶']
    return all(field in data for field in required_fields) and data.get('ç•¶æœˆç‡Ÿæ”¶') != ''

def save_to_file_cache(company_id, year, month, data):
    """ä¿å­˜æ•¸æ“šåˆ°å¿«å–"""
    try:
        cache_path = get_cache_path(company_id, year, month)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
    except Exception as e:
        logger.error(f"ä¿å­˜å¿«å–æ™‚å‡ºéŒ¯: {e}")

def load_from_file_cache(company_id, year, month):
    """å¾å¿«å–åŠ è¼‰æ•¸æ“š"""
    cache_path = get_cache_path(company_id, year, month)
    if os.path.exists(cache_path):
        try:
            # æª¢æŸ¥å¿«å–æ–‡ä»¶æ˜¯å¦éæœŸï¼ˆ30å¤©ï¼‰
            if time.time() - os.path.getmtime(cache_path) < 30 * 24 * 60 * 60:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.error(f"è®€å–å¿«å–æ™‚å‡ºéŒ¯: {e}")
    return None

# ä½¿ç”¨é€€é¿ç­–ç•¥çš„è«‹æ±‚å‡½æ•¸
def fetch_url(url, timeout=30):  # å¢åŠ é»˜èªè¶…æ™‚æ™‚é–“
    """ç²å–URLå…§å®¹ï¼Œå¸¶æœ‰é‡è©¦æ©Ÿåˆ¶ã€é€€é¿ç­–ç•¥å’Œæ›´å¯¬é¬†çš„è¶…æ™‚è¨­ç½®"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://mops.twse.com.tw/mops/web/index',
        'Connection': 'keep-alive',
        'Cache-Control': 'max-age=0'
    }
    
    session = requests.Session()  # ä½¿ç”¨æœƒè©±ä¿æŒé€£æ¥
    
    for attempt in range(5):  # 5æ¬¡é‡è©¦æ©Ÿæœƒ
        try:
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„å»¶é²ç­–ç•¥
            base_delay = 1.0  # å¢åŠ åŸºæœ¬å»¶é²åˆ°1ç§’
            if attempt > 0:
                # æŒ‡æ•¸é€€é¿ + éš¨æ©ŸæŠ–å‹•
                jitter = random.uniform(0.5, 1.5)
                delay = base_delay * (2 ** attempt) * jitter
                time.sleep(delay)
            
            response = session.get(url, timeout=timeout, headers=headers)
            response.raise_for_status()
            
            # æª¢æŸ¥å…§å®¹æ˜¯å¦æœ‰æ•ˆ (é¿å…ç²å–åˆ°éŒ¯èª¤é é¢)
            if 'è³‡æ–™åº«æŸ¥è©¢' in response.text or 'æŠ±æ­‰ï¼Œæ‚¨è¦æ±‚çš„ç¶²é å‡ºç¾éŒ¯èª¤' in response.text:
                if attempt == 4:
                    logger.error(f"ç¶²ç«™è¿”å›éŒ¯èª¤é é¢: {url}")
                    return None
                continue  # é‡è©¦
                
            return response.text
            
        except requests.RequestException as e:
            logger.warning(f"ç¬¬{attempt+1}æ¬¡è«‹æ±‚å¤±æ•—: {url}, éŒ¯èª¤: {e}")
            if attempt == 4:  # æœ€å¾Œä¸€æ¬¡å˜—è©¦
                logger.error(f"è«‹æ±‚å¤±æ•—: {url}, éŒ¯èª¤: {e}")
                return None

def get_company_basic_data(company_id, year, month, html_content):
    """å¾HTMLå…§å®¹è§£æç‰¹å®šå…¬å¸çš„æ•¸æ“š"""
    if not html_content:
        return {}

    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        target_table = soup.find('table')

        if target_table:
            rows = target_table.find_all('tr')[2:]  # å¿½ç•¥å‰å…©è¡Œ

            for row in rows:
                columns = row.find_all('td')
                if columns:
                    fetched_company_id = columns[0].text.strip()
                    if fetched_company_id == company_id:
                        company_name = columns[1].text.strip().encode('latin-1').decode('big5', 'ignore')
                        monthly_revenue = columns[2].text.strip()
                        last_month_revenue = columns[3].text.strip()
                        last_year_month_revenue = columns[4].text.strip()
                        monthly_growth_rate = columns[5].text.strip()
                        last_year_growth_rate = columns[6].text.strip()

                        data = {
                            'å…¬å¸ä»£è™Ÿ': fetched_company_id,
                            'å…¬å¸åç¨±': company_name,
                            'ç•¶æœˆç‡Ÿæ”¶': monthly_revenue,
                            'ä¸Šæœˆç‡Ÿæ”¶': last_month_revenue,
                            'å»å¹´ç•¶æœˆç‡Ÿæ”¶': last_year_month_revenue,
                            'ä¸Šæœˆæ¯”è¼ƒå¢æ¸›(%)': monthly_growth_rate,
                            'å»å¹´åŒæœˆå¢æ¸›(%)': last_year_growth_rate,
                            'æœˆä»½': f'{year}-{month:02d}'
                        }
                        
                        return data
    except Exception as e:
        logger.error(f"è§£ææ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return {}

def process_company_data(args):
    """çµ±ä¸€å…¥å£ï¼šè™•ç†å–®ä¸€å…¬å¸æŸæœˆè³‡æ–™ï¼ˆå«å¿«å–/çˆ¬å–/å…¥åº«ï¼‰"""
    company_id, year, month = args
    update_company(company_id, year, month)

    data = (
        load_valid_cache_or_db(company_id, year, month) or
        fetch_and_process(company_id, year, month)
    )

    increment()
    return data


def load_valid_cache_or_db(company_id, year, month):
    """å…ˆå¾å¿«å–ï¼Œå†å¾è³‡æ–™åº«è®€å–è³‡æ–™"""
    
    # æª¢æŸ¥ JSON å¿«å–æª”æ¡ˆ
    cached = load_from_file_cache(company_id, year, month)
    if cached and validate_cache_data(cached):
        logger.info(f"âœ… ä½¿ç”¨å¿«å–æª”æ¡ˆï¼š{company_id} {year}/{month}")
        throttler.report_success()
        return cached

    # æª¢æŸ¥ SQLite è³‡æ–™åº«
    db_data = db.get_revenue_data(company_id, year, month)
    if db_data and validate_cache_data(db_data):
        logger.info(f"ğŸ“¦ ä½¿ç”¨è³‡æ–™åº«å¿«å–ï¼š{company_id} {year}/{month}")
        throttler.report_success()
        return db_data

    # å¿«å–èˆ‡è³‡æ–™åº«çš†ç„¡æ•ˆ
    return None


def fetch_and_process(company_id, year, month):
    """ç„¡å¿«å–æ™‚ï¼Œé€²è¡ŒæŠ“å– + è§£æ + å…¥åº«"""
    url = f"https://mopsov.twse.com.tw/nas/t21/sii/t21sc03_{year}_{month}_0.html"
    logger.info(f"ğŸŒ é–‹å§‹çˆ¬èŸ²ï¼š{company_id} {year}/{month}")

    html = fetch_url(url)
    if not html:
        logger.warning(f"âŒ æŠ“å–å¤±æ•—ï¼š{company_id} {year}/{month}")
        throttler.report_failure()
        return None

    data = get_company_basic_data(company_id, year, month, html)
    if not data:
        logger.warning(f"âš ï¸ è§£æçµæœç‚ºç©ºï¼š{company_id} {year}/{month}")
        throttler.report_failure()
        return None

    # âœ… å¯«å…¥å¿«å–èˆ‡è³‡æ–™åº«
    save_to_file_cache(company_id, year, month, data)
    db.insert_revenue_data(company_id, year, month, data)
    throttler.report_success()

    return data

# ä¿®æ”¹ get_company_data å‡½æ•°
def get_company_data(company_ids, year_range, month_range):
    """å¹¶è¡ŒæŠ“å–æŒ‡å®šå…¬å¸åœ¨æŒ‡å®šå¹´æœˆèŒƒå›´å†…çš„æ•°æ®"""
    # åˆå§‹åŒ–è¿›åº¦è¿½è¸ª
    total_tasks = len(company_ids) * len(year_range) * len(month_range)
    initialize(total_tasks)
    
    # ç”Ÿæˆæ‰€æœ‰ä»»åŠ¡å‚æ•°
    tasks = [(company_id, year, month) for company_id in company_ids 
             for year in year_range for month in month_range]
    
    results = []
    
    try:
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œï¼Œä½†é™åˆ¶åŒæ—¶æ‰§è¡Œçš„ä»»åŠ¡æ•°é‡
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {executor.submit(process_company_data, task): task for task in tasks}
            
            # æ”¶é›†ç»“æœ
            for future in future_to_task:
                try:
                    data = future.result()
                    if data:
                        results.append(data)
                except Exception as e:
                    logger.error(f"å¤„ç†ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        logger.info(f"æŠ“å–å®Œæˆï¼Œå…±è·å– {len(results)} ç­†æ•°æ®")
        
        # æ ‡è®°ä»»åŠ¡å®Œæˆ
        complete()
        
        return results
    except Exception as e:
        logger.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        
        # æ ‡è®°å‘ç”Ÿé”™è¯¯
        error(str(e))
        
        raise e

# å¯¹å¤–æä¾›è·å–çŠ¶æ€çš„å‡½æ•°
def get_scraper_status():
    """è·å–å½“å‰çˆ¬è™«çŠ¶æ€"""
    return get_status()